---
title: "Functions from research project"
author: "Ma Mengyuan"
date: "`r Sys.Date()`"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Functions from research project}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r setup, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```


## 1.DA

### Introduction

Document `DA`, whose full name is "discriminant analysis", is written by R languague. The R document contains two algorithms, including LDA and QDA.

### Basic idea

+ **LDA**

Suppose that we model each class density as multivariate Gaussian
$$f_k(x) = \frac{1}{(2π)^{p/2}|\Sigma_k|^{1/2}}e^{−\frac{1}{2} (x−\mu_k)^T\Sigma^{−1}_k(x-\mu_k)}\qquad\qquad\qquad\qquad(1)$$

Linear discriminant analysis (LDA) arises in the special case when we assume that the classes have a common covariance matrix $\Sigma_k = \Sigma, \forall k$. In comparing two classes $k$ and $l$, it is sufficient to look at the log-ratio, and we see that
$$\begin{aligned} 
&log \frac{Pr(G = k|X = x)}{Pr(G = l|X = x)} 
\\=& log\frac{f_k(x)}{f_k(x)} + log \frac{\pi_k}{\pi_l} \qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\quad(2)\\
=& log\frac{π_k}{π_l} − \frac{1}{2}(\mu_k + \mu_l)^T\Sigma^{−1}(\mu_k − \mu_l) + x^T \Sigma^{−1}(\mu_k − \mu_l)
\end{aligned} $$
an equation linear in $x$. Therefore, the linear discriminant functions
$$\delta_k(x) = x^T \Sigma^{−1}\mu_k − \frac{1}{2}\mu^T_k \Sigma^{−1}\mu_k + log \pi_k\qquad\qquad\qquad\qquad\quad(3)$$
are an equivalent description of the decision rule, with $G(x)=argmax _k\delta_k(x)$.

+ **QDA**

 Getting back to the general discriminant problem (1), if the $\Sigma_k$ are not assumed to be equal, then the convenient cancellations in (2) do not
occur; in particular the pieces quadratic in $x$ remain. We then get quadratic discriminant functions (QDA),
$$\delta_k(x) = −\frac12 log |\Sigma_k| − \frac12(x − \mu_k)^T \Sigma^{−1}_k (x − \mu_k) + log \pi_k\qquad\quad(4)$$

## Example
```{r}
library(StatComp20060)
library(MASS)
library(caret)
library(ggplot2)
set.seed(1234)
n0 <- sample(1 : 150, 50)
iris.train <- iris[-n0, ]
iris.test <- iris[n0, ]
ptr <- ggplot(data = iris.train, aes(Petal.Length, Petal.Width, color = Species))
ptr <- ptr + geom_point() + theme_bw() + theme(panel.grid = element_blank())
ptr
##LDA
iris.lda <- lda.method(iris.train[, -5], iris.train[, 5],c(1, 1, 1)/3)
pre.lda <- lda.predict(iris.lda, iris.test[, 1 : 4])
print(iris.lda)
#misclassification error
sum(pre.lda != iris.test[, 5])/length(iris.test[, 5])
confusionMatrix(pre.lda, iris.test[, 5])$table
iris.LDA <- lda(Species~., data = iris.train, prior = c(1,1,1)/3)
pre.LDA <- predict(iris.LDA, newdata = iris.test)
#compare with lda
sum(pre.lda != pre.LDA$class)
##QDA
iris.qda <- qda.method(iris.train[, -5], iris.train[, 5],c(35/100, 35/100, 30/100))
pre.qda <- qda.predict(iris.qda, iris.test[, 1 : 4])
#misclassification error
sum(pre.qda != iris.test[, 5])/length(iris.test[, 5])
confusionMatrix(pre.qda, iris.test[, 5])$table
#compare with qda
iris.QDA <- qda(Species~., data = iris.train)
pre.QDA <- predict(iris.QDA, newdata = iris.test)
sum(pre.qda != pre.QDA$class)
```

## 2.PI

### Introduction
Document `PI` is written by C++ languague and contains one function.It basically calculates PI using the Monte Carlo method.

### Basic idea
$$\frac{Area\ of\ Circle}{Area\ of\ Square}=\frac{\pi r^2}{(2r)^2}=\frac{\pi}{4}$$

```{r}
library(StatComp20060)
pi <- piSugar(1000)
print(pi)
```




## References

Hastie T , Tibshirani R , Friedman J . The elements of statistical learning. 2001[J]. Journal of the Royal Statistical Society, 2004.